{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: packaging in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: requests in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from requests->kagglehub) (2024.12.14)\n",
      "Requirement already satisfied: colorama in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from pandas) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from scikit-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\code\\data science\\projects\\brazilian-e-commerce-nlp-deep-learning\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "olist_customers_dataset.csv\n",
    "olist_geolocation_dataset.csv\n",
    "olist_orders_dataset.csv\n",
    "olist_order_items_dataset.csv\n",
    "olist_order_payments_dataset.csv\n",
    "olist_order_reviews_dataset.csv\n",
    "olist_products_dataset.csv\n",
    "olist_sellers_dataset.csv\n",
    "product_category_name_translation.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\darth\\.cache\\kagglehub\\datasets\\olistbr\\brazilian-ecommerce\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"olistbr/brazilian-ecommerce\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the files\n",
    "# olist_customer = pd.read_csv(path + 'olist_customers_dataset.csv')\n",
    "# olist_geolocation = pd.read_csv(path + 'olist_geolocation_dataset.csv')\n",
    "# olist_orders = pd.read_csv(path + 'olist_orders_dataset.csv')\n",
    "# olist_order_items = pd.read_csv(path + 'olist_order_items_dataset.csv')\n",
    "# olist_order_payments = pd.read_csv(path + 'olist_order_payments_dataset.csv')\n",
    "olist_order_reviews = pd.read_csv(path + '\\\\olist_order_reviews_dataset.csv')\n",
    "# olist_products = pd.read_csv(path + 'olist_products_dataset.csv')\n",
    "# olist_sellers = pd.read_csv(path + 'olist_sellers_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>99224.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.086421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.347579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       review_score\n",
       "count  99224.000000\n",
       "mean       4.086421\n",
       "std        1.347579\n",
       "min        1.000000\n",
       "25%        4.000000\n",
       "50%        5.000000\n",
       "75%        5.000000\n",
       "max        5.000000"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olist_order_reviews.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make analysis from datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (40977, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Recebi bem antes do prazo estipulado.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Parabéns lojas lannister adorei comprar pela I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>aparelho eficiente. no site a marca do aparelh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mas um pouco ,travando...pelo valor ta Boa.\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Vendedor confiável, produto ok e entrega antes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                            comment\n",
       "0      5              Recebi bem antes do prazo estipulado.\n",
       "1      5  Parabéns lojas lannister adorei comprar pela I...\n",
       "2      4  aparelho eficiente. no site a marca do aparelh...\n",
       "3      4    Mas um pouco ,travando...pelo valor ta Boa.\\r\\n\n",
       "4      5  Vendedor confiável, produto ok e entrega antes..."
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments = olist_order_reviews.loc[:, ['review_score', 'review_comment_message']]\n",
    "df_comments = df_comments.dropna(subset=['review_comment_message'])\n",
    "df_comments = df_comments.reset_index(drop=True)\n",
    "\n",
    "print(f'Dataset shape: {df_comments.shape}')\n",
    "df_comments.columns = ['score', 'comment']\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40977"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_comments['score']\n",
    "y = y.apply(lambda x: 1 if x < 4 else 0)\n",
    "\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have almost 41k comments that could be used for training a sentimental analysis model. But, beforehand we have to do some preprocessing on the text to transform the comment input into a vector that can be interpreted for a Machine Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_diff(dataset_original, dataset_changed, limit=4):\n",
    "    # Print the original and changed values from lists\n",
    "    count = 0\n",
    "    for original, modified in zip(dataset_original, dataset_changed):\n",
    "        if original != modified:\n",
    "            print(f'Original: {original}')\n",
    "            print(f'Changed:  {modified}')\n",
    "            print('')\n",
    "            count += 1\n",
    "            if count >= limit:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \\n and \\r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as we consider the global internet as the source of our comments, probably we have to deal with some HTML tags, break lines, special characteres and other content that could be part of the dataset. Let's dig a little bit more on Regular Expressions to search for those patterns.\n",
    "\n",
    "First of all, let's define a function that will be used for analysing the results of an applied regular expression. With this we can validate our text pre processing in an easier way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_breakline(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_list: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Applying regex\n",
    "    return [re.sub('[\\n\\r]', ' ', r) for r in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Mas um pouco ,travando...pelo valor ta Boa.\n",
      "\n",
      "Changed:  Mas um pouco ,travando...pelo valor ta Boa.  \n",
      "\n",
      "Original: A compra foi realizada facilmente.\n",
      "A entrega foi efetuada muito antes do prazo dado.\n",
      "O produto já começou a ser usado e até o presente,\n",
      "sem problemas.\n",
      "Changed:  A compra foi realizada facilmente.  A entrega foi efetuada muito antes do prazo dado.  O produto já começou a ser usado e até o presente,  sem problemas.\n",
      "\n",
      "Original: recebi somente 1 controle Midea Split ESTILO.\n",
      "Faltou Controle Remoto para Ar Condicionado Consul\n",
      "Changed:  recebi somente 1 controle Midea Split ESTILO.  Faltou Controle Remoto para Ar Condicionado Consul\n",
      "\n",
      "Original: Ocorreu tudo como contratado sendo a entrega realizada antes do prazo \n",
      " Estou satisfeita\n",
      "\n",
      "Changed:  Ocorreu tudo como contratado sendo a entrega realizada antes do prazo    Estou satisfeita  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a list of comment reviews\n",
    "reviews = list(df_comments['comment'].values)\n",
    "\n",
    "# Applying RegEx\n",
    "reviews_breakline = re_breakline(reviews)\n",
    "df_comments['re_breakline'] = reviews_breakline\n",
    "\n",
    "# Verifying results\n",
    "print_diff(reviews, reviews_breakline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: comprei o produto pela cor ilustrada pelo site da loja americana, no site mostra ser preto http://prntscr.com/jkx7hr quando o produto chegou aqui veio todos com a mesma cor, tabaco http://prntscr.com/\n",
      "Changed:  comprei o produto pela cor ilustrada pelo site da loja americana, no site mostra ser preto  link  quando o produto chegou aqui veio todos com a mesma cor, tabaco  link \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def re_hiperlinks(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_list: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Applying regex\n",
    "    pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    return [re.sub(pattern, ' link ', r) for r in text_list]\n",
    "\n",
    "# Applying RegEx\n",
    "reviews_hiperlinks = re_hiperlinks(reviews_breakline)\n",
    "df_comments['re_hiperlinks'] = reviews_hiperlinks\n",
    "\n",
    "# Printing differences\n",
    "print_diff(reviews_breakline, reviews_hiperlinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: A targaryen não é de confiança não entregou a minha compra e colocou no rastreamento do pedido que foi entregue no dia 14/12/17 empresa falsa quero receber meus produtos que foram pagos com boleto bancari\n",
      "Changed:  A targaryen não é de confiança não entregou a minha compra e colocou no rastreamento do pedido que foi entregue no dia  data  empresa falsa quero receber meus produtos que foram pagos com boleto bancari\n",
      "\n",
      "Original: ENTREGA MUITO DEMORADA, COMPREI EM 26/03/2018 E ATÉ AGORA NÃO RECEBI OS PRODUTOS\n",
      "Changed:  ENTREGA MUITO DEMORADA, COMPREI EM  data  E ATÉ AGORA NÃO RECEBI OS PRODUTOS\n",
      "\n",
      "Original: ainda nao recebi e a ultima informacao sobre p produto e do dia 08/12/2017.\n",
      "Changed:  ainda nao recebi e a ultima informacao sobre p produto e do dia  data .\n",
      "\n",
      "Original: Comprei duas bonecas baby kiss Sid nil dia 07/12/17 e eles entregaram somente 1 boneca ,não veio junto nota fiscal,caixa amassada e também não consigo contato com a lannister o telefone só da ocupado\n",
      "Changed:  Comprei duas bonecas baby kiss Sid nil dia  data  e eles entregaram somente 1 boneca ,não veio junto nota fiscal,caixa amassada e também não consigo contato com a lannister o telefone só da ocupado\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def re_dates(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_list: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Applying regex\n",
    "    pattern = '([0-2][0-9]|(3)[0-1])(\\/|\\.)(((0)[0-9])|((1)[0-2]))(\\/|\\.)\\d{2,4}'\n",
    "    return [re.sub(pattern, ' data ', r) for r in text_list]\n",
    "\n",
    "\n",
    "# Applying RegEx\n",
    "reviews_dates = re_dates(reviews_hiperlinks)\n",
    "df_comments['re_dates'] = reviews_dates\n",
    "\n",
    "# Verifying results\n",
    "print_diff(reviews_hiperlinks, reviews_dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Este foi o pedido  Balde Com 128 Peças - Blocos De Montar 2 un - R$ 25,00 cada (NÃO FOI ENTREGUE)  Vendido e entregue targaryen  Tapete de Eva Nº Letras 36 Peças Crianças 1 un - R$ 35,90 (ESTE FOI ENTREG\n",
      "Changed:  Este foi o pedido  Balde Com 128 Peças - Blocos De Montar 2 un -  dinheiro  cada (NÃO FOI ENTREGUE)  Vendido e entregue targaryen  Tapete de Eva Nº Letras 36 Peças Crianças 1 un -  dinheiro  (ESTE FOI ENTREG\n",
      "\n",
      "Original: Comprei 4 produtos, sendo que só recebi 3. Faltou um lençol branco sem elástico. Foi quase R$ 100,00. E como eu fico? No prejuizo?\n",
      "Changed:  Comprei 4 produtos, sendo que só recebi 3. Faltou um lençol branco sem elástico. Foi quase  dinheiro . E como eu fico? No prejuizo?\n",
      "\n",
      "Original: Relógio belíssimo, muito elegante, inacreditável diante do valor de menos de R$ 150,00! Veio muito bem embrulhado e protegido, fora que tem também caixa muito chique, como se fosse jóia!!! Recomendo\n",
      "Changed:  Relógio belíssimo, muito elegante, inacreditável diante do valor de menos de  dinheiro ! Veio muito bem embrulhado e protegido, fora que tem também caixa muito chique, como se fosse jóia!!! Recomendo\n",
      "\n",
      "Original: Entrega super rápida. Quanto ao produto, não gostei tanto, imaginei que seria melhor e mais bonito, se tivesse visto em uma loja, pegado em mãos antes não teria pagado R$21,90 não, mas ok.\n",
      "Changed:  Entrega super rápida. Quanto ao produto, não gostei tanto, imaginei que seria melhor e mais bonito, se tivesse visto em uma loja, pegado em mãos antes não teria pagado  dinheiro  não, mas ok.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def re_money(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_list: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "\n",
    "    # Applying regex\n",
    "    pattern = '[R]{0,1}\\$[ ]{0,}\\d+(,|\\.)\\d+'\n",
    "    return [re.sub(pattern, ' dinheiro ', r) for r in text_list]\n",
    "\n",
    "# Applying RegEx\n",
    "reviews_money = re_money(reviews_dates)\n",
    "df_comments['re_money'] = reviews_money\n",
    "\n",
    "# Verifying results\n",
    "print_diff(reviews_dates, reviews_money)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: aparelho eficiente. no site a marca do aparelho esta impresso como 3desinfector e ao chegar esta com outro nome...atualizar com a marca correta uma vez que é o mesmo aparelho\n",
      "Changed:  aparelho eficiente. no site a marca do aparelho esta impresso como  numero desinfector e ao chegar esta com outro nome...atualizar com a marca correta uma vez que é o mesmo aparelho\n",
      "\n",
      "Original: Loja nota 10\n",
      "Changed:  Loja nota  numero \n",
      "\n",
      "Original: recebi somente 1 controle Midea Split ESTILO.  Faltou Controle Remoto para Ar Condicionado Consul\n",
      "Changed:  recebi somente  numero  controle Midea Split ESTILO.  Faltou Controle Remoto para Ar Condicionado Consul\n",
      "\n",
      "Original: Este foi o pedido  Balde Com 128 Peças - Blocos De Montar 2 un -  dinheiro  cada (NÃO FOI ENTREGUE)  Vendido e entregue targaryen  Tapete de Eva Nº Letras 36 Peças Crianças 1 un -  dinheiro  (ESTE FOI ENTREG\n",
      "Changed:  Este foi o pedido  Balde Com  numero  Peças - Blocos De Montar  numero  un -  dinheiro  cada (NÃO FOI ENTREGUE)  Vendido e entregue targaryen  Tapete de Eva Nº Letras  numero  Peças Crianças  numero  un -  dinheiro  (ESTE FOI ENTREG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def re_numbers(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_series: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "    \n",
    "    return [re.sub('[0-9]+', ' numero ', r) for r in text_list]\n",
    "\n",
    "reviews_numbers = re_numbers(reviews_money)\n",
    "df_comments['re_numbers'] = reviews_numbers\n",
    "\n",
    "print_diff(reviews_money, reviews_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Não gostei ! Comprei gato por lebre\n",
      "Changed:   negação  gostei ! Comprei gato por lebre\n",
      "\n",
      "Original: Sempre compro pela Internet e a entrega ocorre antes do prazo combinado, que acredito ser o prazo máximo. No stark o prazo máximo já se esgotou e ainda não recebi o produto.\n",
      "Changed:  Sempre compro pela Internet e a entrega ocorre antes do prazo combinado, que acredito ser o prazo máximo. No stark o prazo máximo já se esgotou e ainda  negação  recebi o produto.\n",
      "\n",
      "Original: O produto não chegou no prazo estipulado e causou transtorno, pq programei a viagem de férias do meu filho, baseado no prazo. Moro na Bahia e ele em Cuiabá sozinho. Agora, a casa está vazia. \n",
      "Changed:  O produto  negação  chegou no prazo estipulado e causou transtorno, pq programei a viagem de férias do meu filho, baseado no prazo. Moro na Bahia e ele em Cuiabá sozinho. Agora, a casa está vazia. \n",
      "\n",
      "Original: Produto bom, porém o que veio para mim não condiz com a foto do anúncio.\n",
      "Changed:  Produto bom, porém o que veio para mim  negação  condiz com a foto do anúncio.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def re_negation(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_series: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "    \n",
    "    return [re.sub('([nN][ãÃaA][oO]|[ñÑ]| [nN] )', ' negação ', r) for r in text_list]\n",
    "\n",
    "reviews_negation = re_negation(reviews_numbers)\n",
    "df_comments['re_negation'] = reviews_negation\n",
    "\n",
    "print_diff(reviews_numbers, reviews_negation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Recebi bem antes do prazo estipulado.\n",
      "Changed:  Recebi bem antes do prazo estipulado \n",
      "\n",
      "Original: aparelho eficiente. no site a marca do aparelho esta impresso como  numero desinfector e ao chegar esta com outro nome...atualizar com a marca correta uma vez que é o mesmo aparelho\n",
      "Changed:  aparelho eficiente  no site a marca do aparelho esta impresso como  numero desinfector e ao chegar esta com outro nome   atualizar com a marca correta uma vez que é o mesmo aparelho\n",
      "\n",
      "Original: Mas um pouco ,travando...pelo valor ta Boa.  \n",
      "Changed:  Mas um pouco  travando   pelo valor ta Boa   \n",
      "\n",
      "Original: Vendedor confiável, produto ok e entrega antes do prazo.\n",
      "Changed:  Vendedor confiável  produto ok e entrega antes do prazo \n",
      "\n",
      "Original: GOSTARIA DE SABER O QUE HOUVE, SEMPRE RECEBI E ESSA COMPRA AGORA ME DECPCIONOU\n",
      "Changed:  GOSTARIA DE SABER O QUE HOUVE  SEMPRE RECEBI E ESSA COMPRA AGORA ME DECPCIONOU\n",
      "\n",
      "Original: A compra foi realizada facilmente.  A entrega foi efetuada muito antes do prazo dado.  O produto já começou a ser usado e até o presente,  sem problemas.\n",
      "Changed:  A compra foi realizada facilmente   A entrega foi efetuada muito antes do prazo dado   O produto já começou a ser usado e até o presente   sem problemas \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def re_special_chars(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_series: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "    \n",
    "    return [re.sub('\\W', ' ', r) for r in text_list]\n",
    "\n",
    "reviews_special_chars = re_special_chars(reviews_negation)\n",
    "df_comments['re_special_chars'] = reviews_special_chars\n",
    "\n",
    "print_diff(reviews_negation, reviews_special_chars, limit=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Recebi bem antes do prazo estipulado \n",
      "Changed:  Recebi bem antes do prazo estipulado\n",
      "\n",
      "Original: aparelho eficiente  no site a marca do aparelho esta impresso como  numero desinfector e ao chegar esta com outro nome   atualizar com a marca correta uma vez que é o mesmo aparelho\n",
      "Changed:  aparelho eficiente no site a marca do aparelho esta impresso como numero desinfector e ao chegar esta com outro nome atualizar com a marca correta uma vez que é o mesmo aparelho\n",
      "\n",
      "Original: Mas um pouco  travando   pelo valor ta Boa   \n",
      "Changed:  Mas um pouco travando pelo valor ta Boa\n",
      "\n",
      "Original: Vendedor confiável  produto ok e entrega antes do prazo \n",
      "Changed:  Vendedor confiável produto ok e entrega antes do prazo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def re_whitespaces(text_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text_series: list object with text content to be prepared [type: list]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Applying regex\n",
    "    white_spaces = [re.sub('\\s+', ' ', r) for r in text_list]\n",
    "    white_spaces_end = [re.sub('[ \\t]+$', '', r) for r in white_spaces]\n",
    "    return white_spaces_end\n",
    "\n",
    "reviews_whitespaces = re_whitespaces(reviews_special_chars)\n",
    "df_comments['re_whitespaces'] = reviews_whitespaces\n",
    "\n",
    "# Verifying results\n",
    "print_diff(reviews_special_chars, reviews_whitespaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total portuguese stopwords in the nltk.corpous module: 207\n",
      "['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\darth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Examples of some portuguese stopwords\n",
    "pt_stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "print(f'Total portuguese stopwords in the nltk.corpous module: {len(pt_stopwords)}')\n",
    "print(pt_stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Recebi bem antes do prazo estipulado\n",
      "Changed:  recebi bem antes prazo estipulado\n",
      "\n",
      "Original: Parabéns lojas lannister adorei comprar pela Internet seguro e prático Parabéns a todos feliz Páscoa\n",
      "Changed:  parabéns lojas lannister adorei comprar internet seguro prático parabéns todos feliz páscoa\n",
      "\n",
      "Original: aparelho eficiente no site a marca do aparelho esta impresso como numero desinfector e ao chegar esta com outro nome atualizar com a marca correta uma vez que é o mesmo aparelho\n",
      "Changed:  aparelho eficiente site marca aparelho impresso numero desinfector chegar outro nome atualizar marca correta vez aparelho\n",
      "\n",
      "Original: Mas um pouco travando pelo valor ta Boa\n",
      "Changed:  pouco travando valor ta boa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining a function to remove the stopwords and to lower the comments\n",
    "def stopwords_removal(text, cached_stopwords=nltk.corpus.stopwords.words('portuguese')):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text: list object where the stopwords will be removed [type: list]\n",
    "    cached_stopwords: stopwords to be applied on the process [type: list, default: stopwords.words('portuguese')]\n",
    "    \"\"\"\n",
    "    \n",
    "    return [c.lower() for c in text.split() if c.lower() not in cached_stopwords]\n",
    "\n",
    "reviews_stopwords = [' '.join(stopwords_removal(review)) for review in reviews_whitespaces]\n",
    "df_comments['stopwords_removed'] = reviews_stopwords\n",
    "\n",
    "print_diff(reviews_whitespaces, reviews_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stemming\n",
    "\n",
    "Sure! Stemming is a text normalization process used in Natural Language Processing (NLP) to reduce words to their root or base form. The goal is to group together different forms of a word so they can be analyzed as a single item. For example, the words \"running\", \"runner\", and \"ran\" can all be reduced to the root word \"run\".\n",
    "\n",
    "In summary, stemming helps in reducing words to their base form, which can be useful for various NLP tasks such as text classification, sentiment analysis, and information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\darth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download rslp\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: recebi bem antes prazo estipulado\n",
      "Changed:  receb bem ant praz estipul\n",
      "\n",
      "Original: parabéns lojas lannister adorei comprar internet seguro prático parabéns todos feliz páscoa\n",
      "Changed:  parabém loj lannist ador compr internet segur prát parabém tod feliz pásco\n",
      "\n",
      "Original: aparelho eficiente site marca aparelho impresso numero desinfector chegar outro nome atualizar marca correta vez aparelho\n",
      "Changed:  aparelh efici sit marc aparelh impress numer desinfec cheg outr nom atual marc corret vez aparelh\n",
      "\n",
      "Original: pouco travando valor ta boa\n",
      "Changed:  pouc trav val ta boa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining a function to remove the stopwords and to lower the comments\n",
    "def stemming_process(text, stemmer=nltk.stem.RSLPStemmer()):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ----------\n",
    "    text: list object where the stopwords will be removed [type: list]\n",
    "    stemmer: type of stemmer to be applied [type: class, default: RSLPStemmer()]\n",
    "    \"\"\"\n",
    "    \n",
    "    return [stemmer.stem(c) for c in text.split()]\n",
    "# Applying stemming and looking at some examples\n",
    "reviews_stemmer = [' '.join(stemming_process(review)) for review in reviews_stopwords]\n",
    "df_comments['stemming'] = reviews_stemmer\n",
    "\n",
    "print_diff(reviews_stopwords, reviews_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_final = reviews_stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "After the RegEx, stopwords removal and stemming application, we can use Bag of Words, TF-IDF and Word2Vec to get more meaning. To make our analysis easier, let's define a function that receives a text and a vectorizer object and applies the feature extraction on the respective text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "\n",
    "Count vectorization is a technique in NLP that converts text documents into a matrix of token counts. Tokens can be words, characters, or n-grams. Each token represents a column in the matrix, and the resulting vector for each document has counts for each token.\n",
    "\n",
    "On the Bag of Words approach, we create a dictionary vocabulary with all the unique words and, for each word in each comment/text string, we index the words into a vector that represents the occurrence (1) or not (0) of each word. This is a way for transforming a text into a frequency vector considering a literal bag of words (dictionary vocabulary).\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'and': 0,\n",
       " 'third': 7,\n",
       " 'one': 4}"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document',\n",
    "    'This document is the second document',\n",
    "    'and this is the third one',\n",
    "    'is this the first document'\n",
    "]\n",
    "\n",
    "vec = CountVectorizer().fit(corpus)\n",
    "vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "\n",
    "With the Bag of Words approach, each word has the same weight, which may not be true all the time, especially for those words with a very low frequency in the corpus. So, the TF-IDF (Term Frequency and Inverse Document Frequency) approach can be used with the scikit-learn library following the formulas:\n",
    "\n",
    "$$\n",
    "TF = \\frac{\\text{Frequency of a word in the document}}{\\text{Total words in the document}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "IDF = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing the word}}\\right)\n",
    "$$\n",
    "\n",
    "The purpose of using tf-idf instead of simply counting the frequency of a token in a document is to reduce the influence of tokens that appear very frequently in a given collection of documents. These tokens are less informative than those appearing in only a small fraction of the corpus. Scaling down the impact of these frequently occurring tokens helps improve text-based machine-learning models’ accuracy.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized = vec.transform(corpus).toarray()\n",
    "tfid = TfidfTransformer().fit(vectorized)\n",
    "\n",
    "tfid.transform(vectorized).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [TEXT PREP] Classe para aplicar uma série de funções RegEx definidas em um dicionário\n",
    "class ApplyRegex(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, regex_transformers):\n",
    "        self.regex_transformers = regex_transformers\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Applying all regex functions in the regex_transformers dictionary\n",
    "        for regex_name, regex_function in self.regex_transformers.items():\n",
    "            X = regex_function(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "# [TEXT PREP] Classe para aplicar a remoção de stopwords em um corpus\n",
    "class StopWordsRemoval(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, text_stopwords):\n",
    "        self.text_stopwords = text_stopwords\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return [' '.join(stopwords_removal(comment, self.text_stopwords)) for comment in X]\n",
    "\n",
    "\n",
    "# [TEXT PREP] Classe para aplicar o processo de stemming em um corpus\n",
    "class StemmingProcess(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stemmer):\n",
    "        self.stemmer = stemmer\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return [' '.join(stemming_process(comment, self.stemmer)) for comment in X]\n",
    "\n",
    "\n",
    "# [TEXT PREP] Classe para extração de features de um corpus (vocabulário / bag of words / TF-IDF)\n",
    "class TextFeatureExtraction(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, vectorizer, train=True):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.train = train\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.train:\n",
    "            return self.vectorizer.fit_transform(X).toarray()\n",
    "        else:\n",
    "            return self.vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining regex transformers to be applied\n",
    "regex_transformers = {\n",
    "    'break_line': re_breakline,\n",
    "    'hiperlinks': re_hiperlinks,\n",
    "    'dates': re_dates,\n",
    "    'money': re_money,\n",
    "    'numbers': re_numbers,\n",
    "    'negation': re_negation,\n",
    "    'special_chars': re_special_chars,\n",
    "    'whitespaces': re_whitespaces\n",
    "}\n",
    "\n",
    "# Defining the vectorizer to extract features from text\n",
    "vectorizer = TfidfVectorizer(max_features=300, min_df=7, max_df=0.8, stop_words=pt_stopwords)\n",
    "\n",
    "# Building the Pipeline\n",
    "text_pipeline = Pipeline([\n",
    "    ('regex', ApplyRegex(regex_transformers)),\n",
    "    ('stopwords', StopWordsRemoval(nltk.corpus.stopwords.words('portuguese'))),\n",
    "    ('stemming', StemmingProcess(nltk.stem.RSLPStemmer())),\n",
    "    ('text_features', TextFeatureExtraction(vectorizer))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40977,)"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_comments['comment']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40977, 300)"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed = text_pipeline.fit_transform(X)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Naive Bayes\n",
      "Accuracy: 0.872132747681796\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90      5235\n",
      "           1       0.82      0.83      0.82      2961\n",
      "\n",
      "    accuracy                           0.87      8196\n",
      "   macro avg       0.86      0.86      0.86      8196\n",
      "weighted avg       0.87      0.87      0.87      8196\n",
      "\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.8886041971693509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91      5235\n",
      "           1       0.85      0.84      0.84      2961\n",
      "\n",
      "    accuracy                           0.89      8196\n",
      "   macro avg       0.88      0.88      0.88      8196\n",
      "weighted avg       0.89      0.89      0.89      8196\n",
      "\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Accuracy: 0.8966569058077111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      5235\n",
      "           1       0.86      0.86      0.86      2961\n",
      "\n",
      "    accuracy                           0.90      8196\n",
      "   macro avg       0.89      0.89      0.89      8196\n",
      "weighted avg       0.90      0.90      0.90      8196\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access the corpus and target variables\n",
    "# X = data.review\n",
    "# y = data.sentiment.replace({1:'Negative', 2:'Positive'})\n",
    "\n",
    "# train test splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.20, random_state=0)\n",
    "\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Support Vector Machine\": SVC()\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# construct the pipeline with the procedural steps to\n",
    "# process the data and cast predictions\n",
    "# pipe = Pipeline([\n",
    "#   ('vec', CountVectorizer(stop_words='english', min_df=1000, preprocessor=preprocessor)),\n",
    "#   ('tfid', TfidfTransformer()),\n",
    "#   ('lr', SGDClassifier(loss='log'))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining transformers for preparing the text input\n",
    "# model = clf_tool.classifiers_info['LogisticRegression']['estimator']\n",
    "# prod_pipeline = Pipeline([\n",
    "#     ('regex', ApplyRegex(regex_transformers)),\n",
    "#     ('stopwords', StopWordsRemoval(stopwords.words('portuguese'))),\n",
    "#     ('stemming', StemmingProcess(RSLPStemmer()))\n",
    "# ])\n",
    "# vectorizer = text_pipeline.named_steps['text_features'].vectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
